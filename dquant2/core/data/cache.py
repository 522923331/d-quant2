"""æ•°æ®ç¼“å­˜æ¨¡å—

å®ç°åŸºäº Parquet çš„æœ¬åœ°æ–‡ä»¶ç¼“å­˜ï¼ŒåŠ é€Ÿæ•°æ®è¯»å–
"""

import os
import pandas as pd
import logging
from datetime import datetime, timedelta
from typing import Optional
from pathlib import Path

logger = logging.getLogger(__name__)


class ParquetCache:
    """Parquet æœ¬åœ°ç¼“å­˜ç®¡ç†å™¨
    
    å°†è‚¡ç¥¨æ•°æ®ç¼“å­˜ä¸º Parquet æ–‡ä»¶ï¼ŒæŒ‰è‚¡ç¥¨ä»£ç å­˜å‚¨
    """
    
    def __init__(self, cache_dir: str = "data/cache"):
        """åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•ï¼Œé»˜è®¤ä¸º data/cache
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
    
    def _get_cache_path(self, symbol: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return self.cache_dir / f"{symbol}.parquet"
    
    def load(self, symbol: str, start_date: str, end_date: str) -> Optional[pd.DataFrame]:
        """ä»ç¼“å­˜åŠ è½½æ•°æ®
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ YYYYMMDD æˆ– YYYY-MM-DD
            end_date: ç»“æŸæ—¥æœŸ YYYYMMDD æˆ– YYYY-MM-DD
            
        Returns:
            å¦‚æœç¼“å­˜å­˜åœ¨ä¸”è¦†ç›–è¯·æ±‚çš„æ—¶é—´èŒƒå›´ï¼Œè¿”å› DataFrameï¼›å¦åˆ™è¿”å› None
        """
        file_path = self._get_cache_path(symbol)
        if not file_path.exists():
            logger.debug(f"ç¼“å­˜æœªå‘½ä¸­ {symbol}: æ–‡ä»¶ä¸å­˜åœ¨")
            return None
        
        try:
            # è¯»å– Parquet æ–‡ä»¶
            df = pd.read_parquet(file_path)
            
            # ç¡®ä¿ç´¢å¼•æ˜¯æ—¥æœŸæ—¶é—´ç±»å‹
            if not isinstance(df.index, pd.DatetimeIndex):
                df.index = pd.to_datetime(df.index)
            
            # æ ‡å‡†åŒ–è¯·æ±‚æ—¥æœŸï¼ˆå…¼å®¹ä¸¤ç§æ ¼å¼ï¼‰
            req_start = pd.to_datetime(start_date, format='mixed').normalize()
            req_end = pd.to_datetime(end_date, format='mixed').normalize()
            
            # æ£€æŸ¥ç¼“å­˜æ•°æ®çš„æ—¶é—´èŒƒå›´
            cache_start = df.index.min()
            cache_end = df.index.max()
            
            logger.info(f"ğŸ“¦ ç¼“å­˜æ£€æŸ¥ {symbol}: è¯·æ±‚ {req_start.date()} ~ {req_end.date()}, ç¼“å­˜ {cache_start.date()} ~ {cache_end.date()}")
            
            # è®¡ç®—è¦†ç›–ç‡ï¼šç¼“å­˜æ˜¯å¦å®Œå…¨è¦†ç›–è¯·æ±‚èŒƒå›´
            # ç­–ç•¥ï¼šå¦‚æœç¼“å­˜çš„èµ·å§‹æ—¥æœŸ <= è¯·æ±‚èµ·å§‹ï¼Œä¸”ç¼“å­˜çš„ç»“æŸæ—¥æœŸ >= è¯·æ±‚ç»“æŸï¼Œè®¤ä¸ºå®Œå…¨è¦†ç›–
            # è€ƒè™‘åˆ°äº¤æ˜“æ—¥çš„ä¸è¿ç»­æ€§ï¼Œæˆ‘ä»¬å…è®¸ä¸€å®šçš„å®¹å·®
            fully_covered = (cache_start <= req_start) and (cache_end >= req_end)
            
            if not fully_covered:
                # è®¡ç®—å®é™…æ•°æ®å¯ç”¨æ€§
                mask = (df.index >= req_start) & (df.index <= req_end)
                available_data = df.loc[mask]
                
                if available_data.empty:
                    logger.info(f"âŒ ç¼“å­˜æ— æ•ˆ {symbol}: è¯·æ±‚èŒƒå›´å®Œå…¨åœ¨ç¼“å­˜å¤–")
                    return None
                
                # æœ‰éƒ¨åˆ†æ•°æ®ï¼Œè®¡ç®—è¦†ç›–ç‡
                # ç®€å•ç­–ç•¥ï¼šå¦‚æœç¼“å­˜æ•°æ®å°‘äºè¯·æ±‚èŒƒå›´çš„70%ï¼Œè®¤ä¸ºä¸å¤Ÿï¼Œè¿”å›Noneè§¦å‘å®Œæ•´ä¸‹è½½
                # è¿™é‡Œç”¨å¤©æ•°ä¼°ç®—ï¼ˆå®é™…äº¤æ˜“æ—¥ä¼šæ›´å°‘ï¼‰
                requested_days = (req_end - req_start).days
                available_days = (available_data.index.max() - available_data.index.min()).days
                
                coverage_ratio = available_days / max(requested_days, 1)
                
                logger.info(f"âš ï¸  éƒ¨åˆ†ç¼“å­˜ {symbol}: è¦†ç›–ç‡ {coverage_ratio:.1%} ({len(available_data)}æ¡/{requested_days}å¤©)")
                
                # å¦‚æœè¦†ç›–ç‡å¤ªä½ï¼Œè¿”å›Noneè§¦å‘é‡æ–°ä¸‹è½½
                if coverage_ratio < 0.7:
                    logger.info(f"âŒ ç¼“å­˜è¦†ç›–ç‡ä¸è¶³ {symbol}: {coverage_ratio:.1%} < 70%")
                    return None
            
            # è¿”å›è¯·æ±‚èŒƒå›´å†…çš„æ•°æ®
            mask = (df.index >= req_start) & (df.index <= req_end)
            sliced_df = df.loc[mask]
            
            if sliced_df.empty:
                logger.info(f"âŒ ç¼“å­˜æ— æ•ˆ {symbol}: åˆ‡ç‰‡åæ— æ•°æ®")
                return None
            
            logger.info(f"âœ… ç¼“å­˜å‘½ä¸­ {symbol}: è¿”å› {len(sliced_df)} æ¡æ•°æ®")
            return sliced_df
            
        except Exception as e:
            logger.warning(f"è¯»å–ç¼“å­˜å¤±è´¥ {symbol}: {e}")
            return None
    
    def save(self, symbol: str, df: pd.DataFrame):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            df: è‚¡ç¥¨æ•°æ® DataFrame
        """
        if df is None or df.empty:
            return
            
        file_path = self._get_cache_path(symbol)
        
        try:
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆå¹¶æ•°æ®
            if file_path.exists():
                try:
                    existing_df = pd.read_parquet(file_path)
                    # åˆå¹¶å¹¶å»é‡
                    combined_df = pd.concat([existing_df, df])
                    combined_df = combined_df[~combined_df.index.duplicated(keep='last')]
                    combined_df.sort_index(inplace=True)
                    df = combined_df
                except Exception as e:
                    logger.warning(f"åˆå¹¶ç¼“å­˜å¤±è´¥ {symbol}, å°†è¦†ç›–: {e}")
            
            # ä¿å­˜ä¸º Parquet
            df.to_parquet(file_path, compression='snappy')
            logger.debug(f"å·²ç¼“å­˜ {symbol} æ•°æ®: {len(df)} æ¡")
            
        except Exception as e:
            logger.error(f"å†™å…¥ç¼“å­˜å¤±è´¥ {symbol}: {e}")

    def clear(self, symbol: Optional[str] = None):
        """æ¸…é™¤ç¼“å­˜
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç ï¼Œå¦‚æœä¸ºNoneåˆ™æ¸…é™¤æ‰€æœ‰ç¼“å­˜
        """
        if symbol:
            file_path = self._get_cache_path(symbol)
            if file_path.exists():
                os.remove(file_path)
                logger.info(f"ğŸ—‘ï¸  å·²æ¸…é™¤ {symbol} çš„ç¼“å­˜")
        else:
            # æ¸…é™¤æ‰€æœ‰
            count = 0
            for f in self.cache_dir.glob("*.parquet"):
                os.remove(f)
                count += 1
            logger.info(f"ğŸ—‘ï¸  å·²æ¸…é™¤æ‰€æœ‰ç¼“å­˜ ({count} ä¸ªæ–‡ä»¶)")
    
    def get_cache_info(self, symbol: str) -> Optional[dict]:
        """è·å–æŒ‡å®šè‚¡ç¥¨çš„ç¼“å­˜ä¿¡æ¯
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            
        Returns:
            ç¼“å­˜ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«ï¼šæ–‡ä»¶å¤§å°ã€æ•°æ®æ¡æ•°ã€æ—¥æœŸèŒƒå›´ç­‰
        """
        file_path = self._get_cache_path(symbol)
        if not file_path.exists():
            return None
        
        try:
            df = pd.read_parquet(file_path)
            if not isinstance(df.index, pd.DatetimeIndex):
                df.index = pd.to_datetime(df.index)
            
            file_size = os.path.getsize(file_path)
            
            return {
                'symbol': symbol,
                'file_path': str(file_path),
                'file_size': file_size,
                'file_size_mb': file_size / (1024 * 1024),
                'rows': len(df),
                'columns': list(df.columns),
                'start_date': df.index.min(),
                'end_date': df.index.max(),
                'days_span': (df.index.max() - df.index.min()).days
            }
        except Exception as e:
            logger.error(f"è·å–ç¼“å­˜ä¿¡æ¯å¤±è´¥ {symbol}: {e}")
            return None
    
    def get_cache_stats(self) -> dict:
        """è·å–ç¼“å­˜ç›®å½•ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        parquet_files = list(self.cache_dir.glob("*.parquet"))
        total_size = sum(f.stat().st_size for f in parquet_files)
        
        return {
            'cache_dir': str(self.cache_dir),
            'total_files': len(parquet_files),
            'total_size_mb': total_size / (1024 * 1024),
            'files': [f.stem for f in parquet_files]
        }
